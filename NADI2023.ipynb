{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://nadi.dlnlp.ai/NADI_2023_main.gif\">"
      ],
      "metadata": {
        "id": "B6kLXqlXHERx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install requirments"
      ],
      "metadata": {
        "id": "Zl-i0WkjHN75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgiIvnxYj4Rx",
        "outputId": "f0e59e7a-53d5-4714-e012-297847db215d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/244.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from evaluate)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Collecting huggingface-hub>=0.7.0 (from evaluate)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, xxhash, portalocker, dill, colorama, sacrebleu, responses, multiprocess, huggingface-hub, transformers, datasets, evaluate, accelerate\n",
            "Successfully installed accelerate-0.21.0 colorama-0.4.6 datasets-2.13.1 dill-0.3.6 evaluate-0.4.0 huggingface-hub-0.16.4 multiprocess-0.70.14 portalocker-2.7.0 responses-0.18.0 sacrebleu-2.3.1 safetensors-0.3.1 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.31.0 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate evaluate transformers sentencepiece sacrebleu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download Finetuning code and training samples"
      ],
      "metadata": {
        "id": "fHr9utpdHVnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Donwload FT code and sample for subtask1\n",
        "!wget https://raw.githubusercontent.com/UBC-NLP/nadi/main/run_NADI2023_country_level.py /content/run_NADI2023_country_level.py\n",
        "!wget https://raw.githubusercontent.com/UBC-NLP/nadi/main/NADI2023_subtast1_sample.tsv NADI2023_subtast1_sample.tsv\n",
        "#FT code and sample for subtask2 and subtask3\n",
        "!wget https://raw.githubusercontent.com/UBC-NLP/nadi/main/run_NADI2023_MT.py /content/run_NADI2023_MT.py\n",
        "!wget https://raw.githubusercontent.com/UBC-NLP/nadi/main/NADI2023_MT_examples.tsv /content/NADI2023_MT_examples.tsv"
      ],
      "metadata": {
        "id": "87zn2SgJ09p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Subtask 1 (Closed Country-level Dialect ID)"
      ],
      "metadata": {
        "id": "unBHDfjWHfSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_NADI2023_country_level.py \\\n",
        "  --model_name_or_path UBC-NLP/MARBERTv2 \\\n",
        "  --train_file NADI2023_subtast1_sample.tsv \\\n",
        "  --validation_file NADI2023_subtast1_sample.tsv \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 256 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 5e-5 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --output_dir /content/subtas1_sample_output --overwrite_output_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIusW9x-1F-Q",
        "outputId": "ae4caec7-2a39-461c-c530-a40c9384f4b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-19 21:50:14.372648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/19/2023 21:50:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-c1dafc58ae422e9a/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 10217.55it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 2283.86it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-c1dafc58ae422e9a/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 365.55it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 757/757 [00:00<00:00, 4.48MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 439/439 [00:00<00:00, 2.59MB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 1.10M/1.10M [00:00<00:00, 1.21MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 112/112 [00:00<00:00, 569kB/s]\n",
            "Downloading pytorch_model.bin: 100% 654M/654M [00:02<00:00, 317MB/s]\n",
            "[WARNING|modeling_utils.py:3331] 2023-07-19 21:50:31,557 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERTv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "{'train_runtime': 6.6417, 'train_samples_per_second': 14.906, 'train_steps_per_second': 0.602, 'train_loss': 2.89024019241333, 'epoch': 1.0}\n",
            "100% 4/4 [00:06<00:00,  1.66s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =     2.8902\n",
            "  train_runtime            = 0:00:06.64\n",
            "  train_samples            =         99\n",
            "  train_samples_per_second =     14.906\n",
            "  train_steps_per_second   =      0.602\n",
            " 92% 12/13 [00:01<00:00,  8.97it/s]/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100% 13/13 [00:01<00:00, 10.03it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.2323\n",
            "  eval_f1                 =     0.1169\n",
            "  eval_loss               =     2.7764\n",
            "  eval_precision          =     0.1731\n",
            "  eval_recall             =     0.1558\n",
            "  eval_runtime            = 0:00:01.46\n",
            "  eval_samples            =         99\n",
            "  eval_samples_per_second =     67.403\n",
            "  eval_steps_per_second   =      8.851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subtask 2 (Closed Dialect to MSA MT) and Subtask 3 (Open Dialect to MSA MT)"
      ],
      "metadata": {
        "id": "qwgfbPKvHpkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_NADI2023_MT.py \\\n",
        "  --model_name_or_path UBC-NLP/AraT5-base \\\n",
        "  --train_file NADI2023_MT_examples.tsv \\\n",
        "  --validation_file NADI2023_MT_examples.tsv \\\n",
        "  --learning_rate 5e-5 \\\n",
        "  --max_target_length 256 --max_source_length 256 \\\n",
        "  --per_device_train_batch_size 8 --per_device_eval_batch_size 8 \\\n",
        "  --output_dir /content/MT_output \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --predict_with_generate --source_column \"source_dialect\" --target_column \"target_msa\" \\\n",
        "  --load_best_model_at_end --metric_for_best_model eval_bleu --greater_is_better True --evaluation_strategy epoch --save_strategy epoch --logging_strategy epoch \\\n",
        "  --do_eval --do_train --save_total_limit 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP9_5YqW5jwv",
        "outputId": "be7cd2a0-1053-49a5-ff87-8156d13fd72a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-19 21:50:57.047566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/19/2023 21:51:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
            "07/19/2023 21:51:00 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/MT_output/runs/Jul19_21-50-59_c8122c9c6118,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=epoch,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=eval_bleu,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=/content/MT_output,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/MT_output,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            ">>>> last_checkpoint= None\n",
            "07/19/2023 21:51:01 - INFO - datasets.builder - Using custom data configuration default-7e13abb1a9b186e9\n",
            "07/19/2023 21:51:01 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/csv\n",
            "07/19/2023 21:51:01 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-7e13abb1a9b186e9/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-7e13abb1a9b186e9/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 16039.40it/s]\n",
            "07/19/2023 21:51:01 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "07/19/2023 21:51:01 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 2372.34it/s]\n",
            "07/19/2023 21:51:01 - INFO - datasets.builder - Generating train split\n",
            "07/19/2023 21:51:01 - INFO - datasets.builder - Generating validation split\n",
            "07/19/2023 21:51:01 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-7e13abb1a9b186e9/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 1082.26it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 541/541 [00:00<00:00, 3.30MB/s]\n",
            "[INFO|configuration_utils.py:712] 2023-07-19 21:51:01,887 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--UBC-NLP--AraT5-base/snapshots/ed49be981b4df4040e83de16fd559e191b87429f/config.json\n",
            "[INFO|configuration_utils.py:768] 2023-07-19 21:51:01,891 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110080\n",
            "}\n",
            "\n",
            "Downloading (…)okenizer_config.json: 100% 81.0/81.0 [00:00<00:00, 440kB/s]\n",
            "[INFO|configuration_utils.py:712] 2023-07-19 21:51:02,369 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--UBC-NLP--AraT5-base/snapshots/ed49be981b4df4040e83de16fd559e191b87429f/config.json\n",
            "[INFO|configuration_utils.py:768] 2023-07-19 21:51:02,370 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110080\n",
            "}\n",
            "\n",
            "Downloading spiece.model: 100% 2.44M/2.44M [00:01<00:00, 1.91MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 98.0/98.0 [00:00<00:00, 639kB/s]\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-07-19 21:51:05,866 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--UBC-NLP--AraT5-base/snapshots/ed49be981b4df4040e83de16fd559e191b87429f/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-07-19 21:51:05,866 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-07-19 21:51:05,866 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-07-19 21:51:05,866 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--UBC-NLP--AraT5-base/snapshots/ed49be981b4df4040e83de16fd559e191b87429f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1839] 2023-07-19 21:51:05,866 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--UBC-NLP--AraT5-base/snapshots/ed49be981b4df4040e83de16fd559e191b87429f/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:712] 2023-07-19 21:51:05,866 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--UBC-NLP--AraT5-base/snapshots/ed49be981b4df4040e83de16fd559e191b87429f/config.json\n",
            "[INFO|configuration_utils.py:768] 2023-07-19 21:51:05,867 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110080\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:295] 2023-07-19 21:51:05,867 >> You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
            "[INFO|configuration_utils.py:712] 2023-07-19 21:51:06,059 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--UBC-NLP--AraT5-base/snapshots/ed49be981b4df4040e83de16fd559e191b87429f/config.json\n",
            "[INFO|configuration_utils.py:768] 2023-07-19 21:51:06,060 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110080\n",
            "}\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 1.13G/1.13G [01:21<00:00, 13.9MB/s]\n",
            "[INFO|modeling_utils.py:2603] 2023-07-19 21:52:29,227 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--UBC-NLP--AraT5-base/snapshots/ed49be981b4df4040e83de16fd559e191b87429f/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:599] 2023-07-19 21:52:29,876 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3329] 2023-07-19 21:52:35,195 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:3337] 2023-07-19 21:52:35,195 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at UBC-NLP/AraT5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "[INFO|modeling_utils.py:2949] 2023-07-19 21:52:35,508 >> Generation config file not found, using a generation config created from the model config.\n",
            "Running tokenizer on train dataset:   0% 0/99 [00:00<?, ? examples/s]07/19/2023 21:52:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7e13abb1a9b186e9/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-4c92ce955f17f9e9.arrow\n",
            "Running tokenizer on validation dataset:   0% 0/99 [00:00<?, ? examples/s]07/19/2023 21:52:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7e13abb1a9b186e9/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-c42e0678be2e5033.arrow\n",
            "Downloading builder script: 100% 8.15k/8.15k [00:00<00:00, 27.6MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1686] 2023-07-19 21:52:39,654 >> ***** Running training *****\n",
            "[INFO|trainer.py:1687] 2023-07-19 21:52:39,654 >>   Num examples = 99\n",
            "[INFO|trainer.py:1688] 2023-07-19 21:52:39,654 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1689] 2023-07-19 21:52:39,654 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1692] 2023-07-19 21:52:39,654 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1693] 2023-07-19 21:52:39,654 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1694] 2023-07-19 21:52:39,654 >>   Total optimization steps = 13\n",
            "[INFO|trainer.py:1695] 2023-07-19 21:52:39,655 >>   Number of trainable parameters = 282,770,688\n",
            "  0% 0/13 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-07-19 21:52:39,668 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 85.2354, 'learning_rate': 0.0, 'epoch': 1.0}\n",
            "100% 13/13 [00:04<00:00,  3.92it/s][INFO|trainer.py:3081] 2023-07-19 21:52:43,978 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3083] 2023-07-19 21:52:43,978 >>   Num examples = 99\n",
            "[INFO|trainer.py:3086] 2023-07-19 21:52:43,978 >>   Batch size = 8\n",
            "[INFO|configuration_utils.py:599] 2023-07-19 21:52:43,983 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n",
            "\n",
            "  0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            " 15% 2/13 [00:00<00:03,  3.39it/s]\u001b[A\n",
            " 23% 3/13 [00:01<00:04,  2.20it/s]\u001b[A\n",
            " 31% 4/13 [00:01<00:04,  1.92it/s]\u001b[A\n",
            " 38% 5/13 [00:02<00:04,  1.84it/s]\u001b[A\n",
            " 46% 6/13 [00:03<00:03,  1.76it/s]\u001b[A\n",
            " 54% 7/13 [00:03<00:03,  1.71it/s]\u001b[A\n",
            " 62% 8/13 [00:04<00:03,  1.64it/s]\u001b[A\n",
            " 69% 9/13 [00:05<00:02,  1.50it/s]\u001b[A\n",
            " 77% 10/13 [00:05<00:02,  1.45it/s]\u001b[A\n",
            " 85% 11/13 [00:06<00:01,  1.42it/s]\u001b[A\n",
            " 92% 12/13 [00:07<00:00,  1.39it/s]\u001b[A\n",
            "100% 13/13 [00:08<00:00,  1.08it/s]\u001b[A>>>>>> {'bleu': 0.0, 'gen_len': 19.0}\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 83.64994812011719, 'eval_bleu': 0.0, 'eval_gen_len': 19.0, 'eval_runtime': 11.0549, 'eval_samples_per_second': 8.955, 'eval_steps_per_second': 1.176, 'epoch': 1.0}\n",
            "100% 13/13 [00:15<00:00,  3.92it/s]\n",
            "100% 13/13 [00:08<00:00,  1.08it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2807] 2023-07-19 21:52:55,034 >> Saving model checkpoint to /content/MT_output/checkpoint-13\n",
            "[INFO|configuration_utils.py:458] 2023-07-19 21:52:55,036 >> Configuration saved in /content/MT_output/checkpoint-13/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-07-19 21:52:55,036 >> Configuration saved in /content/MT_output/checkpoint-13/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-07-19 21:52:59,036 >> Model weights saved in /content/MT_output/checkpoint-13/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-07-19 21:52:59,037 >> tokenizer config file saved in /content/MT_output/checkpoint-13/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-07-19 21:52:59,037 >> Special tokens file saved in /content/MT_output/checkpoint-13/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-07-19 21:52:59,202 >> Copy vocab file to /content/MT_output/checkpoint-13/spiece.model\n",
            "[INFO|trainer.py:1934] 2023-07-19 21:53:20,913 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:2093] 2023-07-19 21:53:20,913 >> Loading best model from /content/MT_output/checkpoint-13 (score: 0.0).\n",
            "{'train_runtime': 42.5244, 'train_samples_per_second': 2.328, 'train_steps_per_second': 0.306, 'train_loss': 85.23537973257211, 'epoch': 1.0}\n",
            "100% 13/13 [00:42<00:00,  3.27s/it]\n",
            "[INFO|trainer.py:2807] 2023-07-19 21:53:22,184 >> Saving model checkpoint to /content/MT_output\n",
            "[INFO|configuration_utils.py:458] 2023-07-19 21:53:22,185 >> Configuration saved in /content/MT_output/config.json\n",
            "[INFO|configuration_utils.py:375] 2023-07-19 21:53:22,185 >> Configuration saved in /content/MT_output/generation_config.json\n",
            "[INFO|modeling_utils.py:1851] 2023-07-19 21:53:27,633 >> Model weights saved in /content/MT_output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2210] 2023-07-19 21:53:27,634 >> tokenizer config file saved in /content/MT_output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2023-07-19 21:53:27,634 >> Special tokens file saved in /content/MT_output/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-07-19 21:53:27,805 >> Copy vocab file to /content/MT_output/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =    85.2354\n",
            "  train_runtime            = 0:00:42.52\n",
            "  train_samples            =         99\n",
            "  train_samples_per_second =      2.328\n",
            "  train_steps_per_second   =      0.306\n",
            "07/19/2023 21:53:27 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:3081] 2023-07-19 21:53:27,838 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3083] 2023-07-19 21:53:27,838 >>   Num examples = 99\n",
            "[INFO|trainer.py:3086] 2023-07-19 21:53:27,838 >>   Batch size = 8\n",
            "100% 13/13 [01:05<00:00,  5.34s/it]>>>>>> {'bleu': 0.0, 'gen_len': 255.0}\n",
            "100% 13/13 [01:06<00:00,  5.08s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_bleu               =        0.0\n",
            "  eval_gen_len            =      255.0\n",
            "  eval_loss               =    83.6499\n",
            "  eval_runtime            = 0:01:12.44\n",
            "  eval_samples            =         99\n",
            "  eval_samples_per_second =      1.367\n",
            "  eval_steps_per_second   =      0.179\n",
            "[INFO|modelcard.py:452] 2023-07-19 21:54:40,523 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 0.0}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M9uyraacDo3p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}